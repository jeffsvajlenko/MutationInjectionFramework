/*
 * Copyright 2004 The Apache Software Foundation
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
/* Generated By:JavaCC: Do not edit this line. StandardTokenizer.java */
using System;
namespace Monodoc.Lucene.Net.Analysis.Standard
{

/// <summary>A grammar-based tokenizer constructed with JavaCC.
///
/// <p> This should be a good tokenizer for most European-language documents.
///
/// <p>Many applications have specific tokenizer needs.  If this tokenizer does
/// not suit your application, please consider copying this source code
/// directory to your project and maintaining your own grammar-based tokenizer.
/// </summary>
public class StandardTokenizer : Monodoc.Lucene.Net.Analysis.Tokenizer
{

    /// <summary>Constructs a tokenizer for this Reader. </summary>
    public StandardTokenizer(System.IO.TextReader reader) : this(new FastCharStream(reader))
    {
        this.input = reader;
    }

    /// <summary>Returns the next token in the stream, or null at EOS.
    /// <p>The returned token's type is set to an element of {@link
    /// StandardTokenizerConstants#tokenImage}.
    /// </summary>
    public override Monodoc.Lucene.Net.Analysis.Token Next()
    {
        Token token = null;
        switch ((jj_ntk == - 1) ? Jj_ntk() : jj_ntk)
        {

        case Monodoc.Lucene.Net.Analysis.Standard.StandardTokenizerConstants.ALPHANUM:
            token = Jj_consume_token(Monodoc.Lucene.Net.Analysis.Standard.StandardTokenizerConstants.ALPHANUM);
            break;

        case Monodoc.Lucene.Net.Analysis.Standard.StandardTokenizerConstants.APOSTROPHE:
            token = Jj_consume_token(Monodoc.Lucene.Net.Analysis.Standard.StandardTokenizerConstants.APOSTROPHE);
            break;

        case Monodoc.Lucene.Net.Analysis.Standard.StandardTokenizerConstants.ACRONYM:
            token = Jj_consume_token(Monodoc.Lucene.Net.Analysis.Standard.StandardTokenizerConstants.ACRONYM);
            break;

        case Monodoc.Lucene.Net.Analysis.Standard.StandardTokenizerConstants.COMPANY:
            token = Jj_consume_token(Monodoc.Lucene.Net.Analysis.Standard.StandardTokenizerConstants.COMPANY);
            break;

        case Monodoc.Lucene.Net.Analysis.Standard.StandardTokenizerConstants.EMAIL:
            token = Jj_consume_token(Monodoc.Lucene.Net.Analysis.Standard.StandardTokenizerConstants.EMAIL);
            break;

        case Monodoc.Lucene.Net.Analysis.Standard.StandardTokenizerConstants.HOST:
            token = Jj_consume_token(Monodoc.Lucene.Net.Analysis.Standard.StandardTokenizerConstants.HOST);
            break;

        case Monodoc.Lucene.Net.Analysis.Standard.StandardTokenizerConstants.NUM:
            token = Jj_consume_token(Monodoc.Lucene.Net.Analysis.Standard.StandardTokenizerConstants.NUM);
            break;

        case Monodoc.Lucene.Net.Analysis.Standard.StandardTokenizerConstants.CJK:
            token = Jj_consume_token(Monodoc.Lucene.Net.Analysis.Standard.StandardTokenizerConstants.CJK);
            break;

        case 0:
            token = Jj_consume_token(0);
            break;

        default:
            jj_la1[0] = jj_gen;
            Jj_consume_token(- 1);
            throw new ParseException();

        }
        if (token.kind == Monodoc.Lucene.Net.Analysis.Standard.StandardTokenizerConstants.EOF)
        {
            {
                if (true)
                    return null;
            }
        }
        else
        {
            {
                if (true)
                    return new Monodoc.Lucene.Net.Analysis.Token(token.image, token.beginColumn, token.endColumn, Monodoc.Lucene.Net.Analysis.Standard.StandardTokenizerConstants.tokenImage[token.kind]);
            }
        }
        throw new System.ApplicationException("Missing return statement in function");
    }

    public StandardTokenizerTokenManager token_source;
    public Token token, jj_nt;
    private int jj_ntk;
    private int jj_gen;
    private int[] jj_la1 = new int[1];
    private static int[] jj_la1_0_Renamed_Field;
    private static void  jj_la1_0()
    {
        jj_la1_0_Renamed_Field = new int[] {0x10ff};
    }

    public StandardTokenizer(CharStream stream)
    {
        token_source = new StandardTokenizerTokenManager(stream);
        token = new Token();
        jj_ntk = -1;
        jj_gen = 0;
        for (int i = 0; i < 1; i++)
            jj_la1[i] = -1;
    }

    public virtual void  ReInit(CharStream stream)
    {
        token_source.ReInit(stream);
        token = new Token();
        jj_ntk = -1;
        jj_gen = 0;
        for (int i = 0; i < 1; i++)
            jj_la1[i] = -1;
    }

    public StandardTokenizer(StandardTokenizerTokenManager tm)
    {
        token_source = tm;
        token = new Token();
        jj_ntk = - 1;
        jj_gen = 0;
        for (int i = 0; i < 1; i++)
            jj_la1[i] = - 1;
    }

    public virtual void  ReInit(StandardTokenizerTokenManager tm)
    {
        token_source = tm;
        token = new Token();
        jj_ntk = - 1;
        jj_gen = 0;
        for (int i = 0; i < 1; i++)
            jj_la1[i] = - 1;
    }

    private Token Jj_consume_token(int kind)
    {
        Token oldToken = token;
        if (oldToken.next != null)
            token = token.next;
        else
            token = token.next = token_source.GetNextToken();
        jj_ntk = - 1;
        if (token.kind == kind)
        {
            jj_gen++;
            return token;
        }
        token = oldToken;
        jj_kind = kind;
        throw GenerateParseException();
    }

    public Token GetNextToken()
    {
        if (token.next != null)
            token = token.next;
        else
            token = token.next = token_source.GetNextToken();
        jj_ntk = - 1;
        jj_gen++;
        return token;
    }

    public Token GetToken(int index)
    {
        Token t = token;
        for (int i = 0; i < index; i++)
        {
            if (t.next != null)
                t = t.next;
            else
                t = t.next = token_source.GetNextToken();
        }
        return t;
    }

    private int Jj_ntk()
    {
        if ((jj_nt = token.next) == null)
            return (jj_ntk = (token.next = token_source.GetNextToken()).kind);
        else
            return (jj_ntk = jj_nt.kind);
    }

    private System.Collections.ArrayList jj_expentries = System.Collections.ArrayList.Synchronized(new System.Collections.ArrayList(10));
    private int[] jj_expentry;
    private int jj_kind = - 1;

    public virtual ParseException GenerateParseException()
    {
        jj_expentries.Clear();
        bool[] la1tokens = new bool[15];
        for (int i = 0; i < 15; i++)
        {
            la1tokens[i] = false;
        }
        if (jj_kind >= 0)
        {
            la1tokens[jj_kind] = true;
            jj_kind = - 1;
        }
        for (int i = 0; i < 1; i++)
        {
            if (jj_la1[i] == jj_gen)
            {
                for (int j = 0; j < 32; j++)
                {
                    if ((jj_la1_0_Renamed_Field[i] & (1 << j)) != 0)
                    {
                        la1tokens[j] = true;
                    }
                }
            }
        }
        for (int i = 0; i < 15; i++)
        {
            if (la1tokens[i])
            {
                jj_expentry = new int[1];
                jj_expentry[0] = i;
                jj_expentries.Add(jj_expentry);
            }
        }
        int[][] exptokseq = new int[jj_expentries.Count][];
        for (int i = 0; i < jj_expentries.Count; i++)
        {
            exptokseq[i] = (int[]) jj_expentries[i];
        }
        return new ParseException(token, exptokseq, Monodoc.Lucene.Net.Analysis.Standard.StandardTokenizerConstants.tokenImage);
    }

    public void  Enable_tracing()
    {
    }

    public void  Disable_tracing()
    {
    }
    static StandardTokenizer()
    {
        {
            jj_la1_0();
        }
    }
}
}
